{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b7a1d6ffc2484b8c08c723797b7775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer,LlamaConfig\n",
    "from datasets import load_dataset\n",
    "import bitsandbytes as bnb\n",
    "# config = LlamaConfig.()\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"/data/zhangyi/hf_model/Llama2-7b-chat-hf\")\n",
    "model  = LlamaForCausalLM.from_pretrained(\"/data/zhangyi/hf_model/Llama2-7b-chat-hf\",device_map=\"cuda:0\"\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: model | Details: LlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      ")\n",
      "Layer: lm_head | Details: Linear(in_features=4096, out_features=32000, bias=False)\n"
     ]
    }
   ],
   "source": [
    "# peft\n",
    "# model.print_trainable_parameters()\n",
    "for name, layer in model.named_children():\n",
    "    print(f'Layer: {name} | Details: {layer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "torch.Size([32000, 4096])\n",
      "True\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "torch.Size([4096, 4096])\n",
      "True\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "torch.Size([4096, 4096])\n",
      "True\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "torch.Size([4096, 4096])\n",
      "True\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "torch.Size([4096, 4096])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for name,parameter in list(model.named_parameters())[:5]:\n",
    "    print(name) \n",
    "    print(parameter.shape)\n",
    "    print(parameter.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 6758404096\n",
      "Trainable parameters: 19988480\n"
     ]
    }
   ],
   "source": [
    "# index = 0\n",
    "# for i in model.parameters():\n",
    "#     index +=1\n",
    "# index\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# peft load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_config, get_peft_model, get_peft_model_state_dict, LoraConfig, TaskType, PeftModel\n",
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM,\n",
    "                        inference_mode=False,\n",
    "                        r=8 ,\n",
    "                        lora_alpha=32,\n",
    "                        lora_dropout=0.1 ,\n",
    "                        target_modules=['k_proj', 'v_proj', 'q_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'])\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 19,988,480 || all params: 6,758,404,096 || trainable%: 0.2957573965106688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19988480"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "peft_model.print_trainable_parameters()\n",
    "-(6738415616  - 6758404096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: base_model | Details: LoraModel(\n",
      "  (model): LlamaForCausalLM(\n",
      "    (model): LlamaModel(\n",
      "      (embed_tokens): Embedding(32000, 4096)\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x LlamaDecoderLayer(\n",
      "          (self_attn): LlamaAttention(\n",
      "            (q_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "            )\n",
      "            (k_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "            )\n",
      "            (v_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "            )\n",
      "            (o_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "            )\n",
      "            (rotary_emb): LlamaRotaryEmbedding()\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            (gate_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=8, out_features=11008, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "            )\n",
      "            (up_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=8, out_features=11008, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "            )\n",
      "            (down_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=11008, out_features=8, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "            )\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm()\n",
      "          (post_attention_layernorm): LlamaRMSNorm()\n",
      "        )\n",
      "      )\n",
      "      (norm): LlamaRMSNorm()\n",
      "    )\n",
      "    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for name, layer in peft_model.named_children():\n",
    "    print(f'Layer: {name} | Details: {layer}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
