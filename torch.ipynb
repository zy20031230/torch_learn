{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch - learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## einsum\n",
    "\n",
    "1. 自由索引，出现在箭头右边的索引，比如上面的例子就是 i 和 j；\n",
    "2. 求和索引，只出现在箭头左边的索引，表示中间计算结果需要这个维度上求和之后才能得到输出，比如上面的例子就是 k；\n",
    "   \n",
    "三条规则\n",
    "规则一，equation 箭头左边，在不同输入之间重复出现的索引表示，把输入张量沿着该维度做乘法操作，比如还是以上面矩阵乘法为例， \"ik,kj->ij\"，k 在输入中重复出现，所以就是把 a 和 b 沿着 k 这个维度作相乘操作；\n",
    "规则二，只出现在 equation 箭头左边的索引，表示中间计算结果需要在这个维度上求和，也就是上面提到的求和索引；\n",
    "规则三，equation 箭头右边的索引顺序可以是任意的，比如上面的 \"ik,kj->ij\" 如果写成 \"ik,kj->ji\"，那么就是返回输出结果的转置，用户只需要定义好索引的顺序，转置操作会在 einsum 内部完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.7030, 4.2929, 1.7279, 1.7012])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor_i = torch.randn(4,4)\n",
    "tensor_i\n",
    "tensor_j = torch.randn(4,4)\n",
    "\n",
    "k = torch.einsum('ii->i', tensor_i)# 对角元素求和\n",
    "\n",
    "k = torch.einsum('ij,ij->',tensor_i,tensor_j)# 向量dot求和\n",
    "\n",
    "k = torch.einsum(\"ij->i\",tensor_i) #保留I的维度\n",
    "\n",
    "k = torch.einsum(\"i,j->ij\",tensor_i,tensor_j) #\n",
    "\n",
    "# 本质上都是在相同的维度上进行运算，然后进行求和。\n",
    "\n",
    "k = torch.einsum('')\n",
    "k\n",
    "\n",
    "from einops import rearrange, reduce #  两种可以重组不同torch的库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### broadcast mechanism\n",
    "可广播的一对张量需满足以下规则：\n",
    "- 每个张量至少有一个维度。\n",
    "- 迭代维度尺寸时，从尾部的维度开始，维度尺寸或者相等，\n",
    "​- 或者其中一个张量的维度尺寸为 1 ，\n",
    "​- 或者其中一个张量不存在这个维度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pad \n",
    "\n",
    "从维度上作为逆序，首先在前面的维度中进行相关的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 3., 3., 3., 3.],\n",
       "         [0., 0., 3., 3., 3., 3.],\n",
       "         [0., 0., 3., 3., 3., 3.]],\n",
       "\n",
       "        [[0., 0., 3., 3., 3., 3.],\n",
       "         [0., 0., 3., 3., 3., 3.],\n",
       "         [0., 0., 3., 3., 3., 3.]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import beartype\n",
    "from beartype import beartype\n",
    "# from beartype import Tensor\n",
    "# F.pad\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch \n",
    "i = torch.full((2,3,4),3.0)\n",
    "\n",
    "# 以上的做法可以把全部的值都当作元祖放置到gpu上面\n",
    "j = F.pad(i,(2,0),mode='constant',value=0)\n",
    "\n",
    "\n",
    "# masked_fill 的做法是，通过加入bool型变量的值，把在true的位置都填入pad_id 进行填充\n",
    "j = j.masked_fill(j== j,0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arange_start_at_token_id(\n",
    "        token_ids: torch.Tensor,\n",
    "        token_id: int,\n",
    "        pad_id = -1):\n",
    "    is_token_id_mask = token_ids == token_id\n",
    "    arange = (is_token_id_mask.cumsum(dim=1) > 0).cumsum(dim=-1)# cumsum是一种累积和的方式\n",
    "    before_token_mask = arange == 0\n",
    "    arange = arange - 1\n",
    "    arange = arange.masked_fill(before_token_mask, pad_id)\n",
    "    return arange # end 之前填充pad_id and after is [0,...]\n",
    "    \n",
    "\n",
    "def default_weight_fn(t):\n",
    "    return (1. - t * 0.2).clamp(min=0.)\n",
    "def weigh_and_mask(\n",
    "        token_ids: torch.Tensor,\n",
    "        token_id: int,\n",
    "        pad_id = -1,\n",
    "        weighting_fn: Callable = default_weight_fn):\n",
    "    t = get_arange_start_at_token_id(token_ids, token_id, pad_id)\n",
    "    weighs = weighting_fn(t)\n",
    "    return weighs.masked_fill(t == pad_id, 0.)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对于张量的操作\n",
    "\n",
    "#### gather\n",
    "\n",
    "gather的想法是从tensor中按照标签选取合适的值。\n",
    "\n",
    "\n",
    "原理是在通过扩散和src—data 中相同的维度然后通过把里面的数据替换到指定的维度中。\n",
    "\n",
    "\n",
    "同时可以选择相应的行向量和列向量的形式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9,  7,  5],\n",
       "        [ 3, 10,  8]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor_0 = torch.arange(3,12).view(3,3)\n",
    "index = torch.tensor([[2,1,0],[0,2,1]])# \n",
    "index = index.view(2,3)\n",
    "tensor_1 = tensor_0.gather(0,index)\n",
    "tensor_1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# pad_sequence 用于序列的填充\n",
    "\n",
    "\n",
    "def pad_sequence(\n",
    "    sequences: Union[Tensor, List[Tensor]],\n",
    "    batch_first: bool = False,\n",
    "    padding_value: float = 0.0,\n",
    ") -> Tensor: \n",
    "pad_sequence = partial(pad_sequence, padding_value = 0) # 可以通过partial 来固定一部分参数。\n",
    "# batch_first 表示是否把batch放在第一维度上面\n",
    "# list 的模式是填充为统一的长度，同时增加batch的长度\n",
    "# padding value 表示为填充的值\n",
    "# 表示要么用tensor 要么用List tensor的模式\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cumsum\n",
    "\n",
    "cumsum 是 PyTorch 中的一个函数，它用于计算输入张量的累积和（cumulative sum）。这个函数会沿着指定的维度计算累积和。\n",
    "\n",
    "在你的代码中，is_token_id_mask.cumsum(dim = -1) 这行代码计算的是 is_token_id_mask 张量在最后一个维度上的累积和。is_token_id_mask 是一个布尔张量，其中的每个元素表示相应的 token id 是否等于指定的 token_id。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [0, 1, 2]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_token_id_mask = torch.tensor([[True, False, True],\n",
    "                                 [False, True, False]], dtype=torch.bool)\n",
    "(is_token_id_mask.cumsum(dim=-1) >0 ).cumsum(dim=-1) # 一个设置的开始的位置，对于后续数据进行测量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regex    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello\\\\ world\\\\\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.findall(r'\\d+','1234')# 采用的是pattern的形式 和匹配字符串的形式\n",
    "re.escape(\"hello world\\n\") # 表示为处理正规式的方式\n",
    "\n",
    "\n",
    "\n",
    "def replace_fn(\n",
    "        matches,\n",
    "        registry,\n",
    "        delimiter = '->'\n",
    "):\n",
    "    orig_text = matches.group(0)\n",
    "    text_without_end_api_token = matches.group(1)\n",
    "    end_api_token = matches.group(4)\n",
    "    function_name = matches.group(2)\n",
    "\n",
    "    if function_name not in registry:\n",
    "        return orig_text\n",
    "    fn = registry[function_name]\n",
    "    params = matches.group(3).splits(',')\n",
    "    params = list(map(lambda x: x.strip(), params))# 好帅的去括号方式\n",
    "    params = list(filter(len,params))\n",
    "    \n",
    "    if any([])\n",
    "    pass\n",
    "\n",
    "filter(function,iterable) # filter 函数的使用方式,通过function判断可迭代变量中的式子是否为真，同时进行返回\n",
    "regex = rf'hello'\n",
    "text = 'hello'\n",
    "re.sub(regex,replace_fn, text) # sub 通过自动匹配比较的方式，从text中获得match，然后传递参数把里面的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader\n",
    "\n",
    "```python\n",
    "class DataLoader(object):\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "                 batch_sampler=None, num_workers=0, collate_fn=default_collate,\n",
    "                 pin_memory=False, drop_last=False, timeout=0,\n",
    "                 worker_init_fn=None)\n",
    "    \n",
    "in : list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n",
    "out: [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n",
    "\n",
    "```\n",
    "以上可以看到两种sample的方式：sampler 和 batch_sampler，默认设置成None，同时batch_sampler的生成是基于sampler的同时按照batchsize进行分组。\n",
    "\n",
    "- shuffle = True sampler=RandomSampler(dataset)\n",
    "- shuffle = False 则sampler=SequentialSampler(dataset)\n",
    "- _如果自定义了Sample_ shuffle需要设置成false\n",
    "-  drop_last 是否能整除全部的数列\n",
    "\n",
    "### collate_fn \n",
    "\n",
    "```python\n",
    "class DataLoader(object): \n",
    "    ... \n",
    "     \n",
    "    def __next__(self): \n",
    "        if self.num_workers == 0:   \n",
    "            indices = next(self.sample_iter)  \n",
    "            batch = self.collate_fn([self.dataset[i] for i in indices]) # this line \n",
    "            if self.pin_memory: \n",
    "                batch = _utils.pin_memory.pin_memory_batch(batch) \n",
    "            return batch\n",
    "            \n",
    "def prompt_collate_fn(data, padding_value = 0):# 应该是直接对于一批的数据进行同步处理\n",
    "    prompts, prompt_lengths = zip(*data)\n",
    "    prompts = pad_sequence(prompts, padding_value = padding_value)\n",
    "    return prompts, torch.stack(prompt_lengths) \n",
    "```\n",
    "\n",
    "如上代码所示，collate_fn把多个数据合称为一个batch数据\n",
    "### sampler\n",
    "\n",
    "sample的各种方法都是继承同一个父类，同时我们只需要负责iter函数的设计，同时他的返回值是可迭代的\n",
    "```python\n",
    "iter(range(len(self.data_source)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 父类\n",
    "\n",
    "class Sampler(object):\n",
    "    r\"\"\"Base class for all Samplers.\n",
    "    Every Sampler subclass has to provide an :meth:`__iter__` method, providing a\n",
    "    way to iterate over indices of dataset elements, and a :meth:`__len__` method\n",
    "    that returns the length of the returned iterators.\n",
    "    .. note:: The :meth:`__len__` method isn't strictly required by\n",
    "              :class:`~torch.utils.data.DataLoader`, but is expected in any\n",
    "              calculation involving the length of a :class:`~torch.utils.data.DataLoader`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source):\n",
    "        pass\n",
    "    def __iter__(self):\n",
    "        raise NotImplementedError\n",
    "    def __len__(self):\n",
    "        raise len(self.data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from beartype import beartype\n",
    "from beartype import Callable,Optional,Union,List,Tuple\n",
    "import re\n",
    "\n",
    "DataLoader(dataset,batchsize= 16,shuffle= True,drop_last = True)\n",
    "class FinetuneData(Dataset):# 数据至少含有len和getitem两个函数\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokens : torch.Tensor\n",
    "    ):\n",
    "        self.tokens = tokens\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem(self, idx):\n",
    "        return self.tokens[idx]\n",
    "    \n",
    "\n",
    "class PromptDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            prompt:str,\n",
    "            prompt_input_tag: str,\n",
    "            data:List[str],\n",
    "            tokenizer_encode: Callable  \n",
    "    ):\n",
    "        self.data = data\n",
    "        self.prompt = prompt\n",
    "        self.prompt_input_tag_regex = re.escape(prompt_input_tag)\n",
    "        self.tokenizer_encode = tokenizer_encode\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem(self,idx):\n",
    "        data_string = self.data[idx]\n",
    "        data_with_prompt = re.sub(self.prompot_input_tag_regex,data_string,self.prompt)\n",
    "        token_ids = self.tokenizer_encode(data_with_prompt)\n",
    "        return torch.tensor(token_ids).long(), torch.tensor(len(token_ids)).long() # 一个作为label应该可以用作之后的处理\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss\n",
    "\n",
    "#### cross Entropy loss\n",
    "\n",
    "$$H(p,q) = -\\sum_x (p(x))\\lg softmax(q(x)) $$\n",
    "\n",
    "p 为真实的值， q为预测值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1142)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "entroy = nn.CrossEntropyLoss()\n",
    "input=torch.Tensor([[0.1234, 0.5555,0.3211],[0.1234, 0.5555,0.3211],[0.1234, 0.5555,0.3211],])\n",
    "target = torch.tensor([0,1,2])\n",
    "loss = entroy(input,target)\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learn from code\n",
    "\n",
    "### tool-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[..., api_start_token_id] = True # ... 表示任意数量的冒号\n",
    "\n",
    "last_logits = logits[batch_indices, position_indices] # 表示在维度上分别用两遍的值进行筛选\n",
    "\n",
    " t.clamp(min = eps).log()  # 小于 eps的值都填上eps ，同时.log（）表示对向量取ln\n",
    "\n",
    "torch.zeros_like(j).uniform_(0, 1) # uniform 采用的正态分布的形式\n",
    "\n",
    "tensor = tensor.type(torch.float) # 表示修改张量的形式\n",
    "\n",
    "tensor.argmax(dim = ) # 返回的是指定维度数上面减一的操作，同时也是在指定的维度上进行统计\n",
    "tensor.any(dim = -1) # 同样的，不过是检查bool型的变量\n",
    "\n",
    "\n",
    "tokens, tokens_without_api_response, tokens_with_api_response = map(lambda t: t.to(device), (tokens, tokens_without_api_response, tokens_with_api_response))\n",
    "\n",
    "tensor.topk(k,dim ) # 在相应的维度上进行操作，同时返回的indices 和 values 两个子元。\n",
    "\n",
    "tensor.item() # 表示的是把tensor转化为标量\n",
    "\n",
    "pad_sequence() #\n",
    "\n",
    "tensor == p # 表示对于每一个维度都需要进行bool的判断 同时如果是完整的传入了一个向量，必须判断是完全相等的形式\n",
    "\n",
    "torch.finfo(last_logits.dtype).max # 表示的寻找序列变量的极值\n",
    "\n",
    "nametuple('name',['','']) # 创建命名类的元祖，可以通过名字来访问相应的标签。\n",
    "\n",
    "zip(*data) # data是多种可迭代数据，zip把每个数据组的头元素压缩在一起成为元组，用list的形式表现出来\n",
    "\n",
    "torch.stack() # 可以把一组向量堆叠起来\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python 语法\n",
    "\n",
    "### dict\n",
    "\n",
    "** dict 表示把键对的值用key= value的方式表现出来传递到函数中 \n",
    "\n",
    "\n",
    "### function\n",
    "\n",
    "```python\n",
    "\n",
    "filter(filter_fn,iterable) \n",
    "\n",
    "any() # 函数接受一个可以迭代的对象，如果里面有任意一个元素为真，则返回true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## huggingface NLP \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logits \n",
    "\n",
    "在NLP任务中，logits表示对于各个模型分类的概率，相对于softmax处理之前的形式，softmax处理之后的形式就是概率的形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification,tokenizer\n",
    "clf = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased',num_labels = 2)\n",
    "inputs = tokenizer(raw_input,padding = True, truncation = True, return_tensors = 'pt')# input 可以作为list放进入\n",
    "outputs = clf(**inputs) \n",
    "\n",
    "输出 ： dict_keys(['loss', 'logits', 'hidden_states', 'attentions'])\n",
    "tensor([[-4.2098,  4.6444],\n",
    "        [ 0.6367, -0.3753]], grad_fn=<AddmmBackward>)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
