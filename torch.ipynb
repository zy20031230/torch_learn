{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch - learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## einsum\n",
    "\n",
    "1. 自由索引，出现在箭头右边的索引，比如上面的例子就是 i 和 j；\n",
    "2. 求和索引，只出现在箭头左边的索引，表示中间计算结果需要这个维度上求和之后才能得到输出，比如上面的例子就是 k；\n",
    "   \n",
    "三条规则\n",
    "规则一，equation 箭头左边，在不同输入之间重复出现的索引表示，把输入张量沿着该维度做乘法操作，比如还是以上面矩阵乘法为例， \"ik,kj->ij\"，k 在输入中重复出现，所以就是把 a 和 b 沿着 k 这个维度作相乘操作；\n",
    "规则二，只出现在 equation 箭头左边的索引，表示中间计算结果需要在这个维度上求和，也就是上面提到的求和索引；\n",
    "规则三，equation 箭头右边的索引顺序可以是任意的，比如上面的 \"ik,kj->ij\" 如果写成 \"ik,kj->ji\"，那么就是返回输出结果的转置，用户只需要定义好索引的顺序，转置操作会在 einsum 内部完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.7030, 4.2929, 1.7279, 1.7012])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor_i = torch.randn(4,4)\n",
    "tensor_i\n",
    "tensor_j = torch.randn(4,4)\n",
    "\n",
    "k = torch.einsum('ii->i', tensor_i)# 对角元素求和\n",
    "\n",
    "k = torch.einsum('ij,ij->',tensor_i,tensor_j)# 向量dot求和\n",
    "\n",
    "k = torch.einsum(\"ij->i\",tensor_i) #保留I的维度\n",
    "\n",
    "k = torch.einsum(\"i,j->ij\",tensor_i,tensor_j) #\n",
    "\n",
    "# 本质上都是在相同的维度上进行运算，然后进行求和。\n",
    "\n",
    "k = torch.einsum('')\n",
    "k\n",
    "\n",
    "from einops import rearrange, reduce #  两种可以重组不同torch的库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.pad\n",
    "\n",
    "从维度上作为逆序，首先在前面的维度中进行相关的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 3., 3., 3., 3.],\n",
       "         [0., 0., 3., 3., 3., 3.],\n",
       "         [0., 0., 3., 3., 3., 3.]],\n",
       "\n",
       "        [[0., 0., 3., 3., 3., 3.],\n",
       "         [0., 0., 3., 3., 3., 3.],\n",
       "         [0., 0., 3., 3., 3., 3.]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch \n",
    "i = torch.full((2,3,4),3.0)\n",
    "\n",
    "# 以上的做法可以把全部的值都当作元祖放置到gpu上面\n",
    "j = F.pad(i,(2,0),mode='constant',value=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regex    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello\\\\ world\\\\\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.findall(r'\\d+','1234')# 采用的是pattern的形式 和匹配字符串的形式\n",
    "re.escape(\"hello world\\n\") # 表示为处理正规式的方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from beartype import beartype\n",
    "from beartype import Callable,Optional,Union,List,Tuple\n",
    "import re\n",
    "class FinetuneData(Dataset):# 数据至少含有len和getitem两个函数\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokens : torch.Tensor\n",
    "    ):\n",
    "        self.tokens = tokens\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem(self, idx):\n",
    "        return self.tokens[idx]\n",
    "    \n",
    "\n",
    "class PromptDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            prompt:str,\n",
    "            prompt_input_tag: str,\n",
    "            data:List[str],\n",
    "            tokenizer_encode: Callable  \n",
    "    ):\n",
    "        self.data = data\n",
    "        self.prompt = prompt\n",
    "        self.prompt_input_tag_regex = re.escape(prompt_input_tag)\n",
    "        self.tokenizer_encode = tokenizer_encode\n",
    "    \n",
    "\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learn from code\n",
    "\n",
    "### tool-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[..., api_start_token_id] = True # ... 表示任意数量的冒号\n",
    "\n",
    "last_logits = logits[batch_indices, position_indices] # 表示在维度上分别用两遍的值进行筛选\n",
    "\n",
    " t.clamp(min = eps).log()  # 小于 eps的值都填上eps ，同时.log（）表示对向量取ln\n",
    "\n",
    "torch.zeros_like(j).uniform_(0, 1) # uniform 采用的正态分布的形式\n",
    "\n",
    "tensor = tensor.type(torch.float) # 表示修改张量的形式\n",
    "\n",
    "tensor.argmax(dim = ) # 返回的是指定维度数上面减一的操作，同时也是在指定的维度上进行统计\n",
    "tensor.any(dim = -1) # 同样的，不过是检查bool型的变量\n",
    "\n",
    "\n",
    "tokens, tokens_without_api_response, tokens_with_api_response = map(lambda t: t.to(device), (tokens, tokens_without_api_response, tokens_with_api_response))\n",
    "\n",
    "tensor.topk(k,dim ) # 在相应的维度上进行操作，同时返回的indices 和 values 两个子元。\n",
    "\n",
    "tensor == p # 表示对于每一个维度都需要进行bool的判断 同时如果是完整的传入了一个向量，必须判断是完全相等的形式\n",
    "\n",
    "torch.finfo(last_logits.dtype).max # 表示的寻找序列变量的极值\n",
    "\n",
    "nametuple('name',['']) # 创建命名类的元祖，可以通过名字来访问相应的标签。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
